from google.colab import drive
drive.mount('/content/drive')-
        This line imports the drive module from the google.colab package. In Google Colab, google.colab provides various utilities
        and  functionalities to interact with the Colab environment, including accessing files, mounting Google Drive, etc.


pip: pip is the package installer for Python. It is used to install and manage Python packages 
      that are available on the Python Package Index (PyPI).

pytesseract--allows Python to interface with Tesseract, an open-source OCR engine maintained by Google. Tesseract is
 capable of recognizing text from images, making it useful for tasks like reading scanned documents,
 extracting text from images, and more.
pytesseract-This library is commonly used for optical character recognition (OCR) 
tasks in Python projects. OCR is the process of extracting text from images or scanned documents.


import shutil- shutil is a Python standard library module that provides functions for file operations,
 such as copying, moving, and deleting files and directories

import os-os module provides a portable way of interacting with the operating system,
 allowing Python code to perform tasks such as:
File and Directory Manipulation: The os module provides functions for creating, deleting, renaming, and listing files and directories.
 In a bank document classification project, you might use these functions to organize, move, or delete files before or after processing.
Path Manipulation: The os.path

random module in Python=
which provides functions for generating random numbers, 
selecting random elements from lists, and shuffling sequences. In a bank document classification project, 
the random module might be used for various purposes:
Data Augmentation: In some cases, it might be beneficial to augment the training data by introducing variations.
 For example, you could use the random module to randomly rotate, flip, or add noise to document images to create additional training samples.

The line of code from PIL import Image -
is used to import the Image module from the Python Imaging Library (PIL)
This module provides functionality for opening, manipulating, and saving many different image file.

except ImportError: import Image: If the from PIL import Image statement fails due to an ImportError (typically because PIL 
might not be installed or accessible), it falls back to importing the Image module directly.
 In older versions of PIL, the module was named Image, so this ensures compatibility with older code bases.



/content/drive/MyDrive/bankdataset/
    ├── category1/
    │   ├── subcategory1/
    │   │   ├── file1.txt
    │   │   ├── file2.txt
    │   │   └── ...
    │   ├── subcategory2/
    │   │   ├── file3.txt
    │   │   ├── file4.txt
    │   │   └── ...
    │   └── ...
    ├── category2/
    │   ├── subcategory3/
    │   │   ├── file5.txt
    │   │   ├── file6.txt
    │   │   └── ...
    │   ├── subcategory4/
    │   │   ├── file7.txt
    │   │   ├── file8.txt
    │   │   └── ...
    │   └── ...
    └── ...

 import glob: 
 This line imports the glob module, which provides a way to retrieve file pathnames matching a specified pattern 
according to the rules used by the Unix shell. It's very useful for finding files that match a specific pattern or wildcard.

files = glob.glob("/content/drive/MyDrive/bankdataset/*/*/*"): 
This line uses the glob.glob() function to search for files within the
directory /content/drive/MyDrive/bankdataset/. The * wildcard character matches any sequence of characters, 
so */*/* matches any files or directories that are two levels deep within bankdataset.
        The first * matches any subdirectories directly under /content/drive/MyDrive/bankdataset/.
        The second * matches any subdirectories within those subdirectories.
        The third * matches any files or directories within the second level of subdirectories.

import numpy- as np imports the numpy library, providing access to its powerful capabilities for 
numerical computations and array manipulations in Python. 
Renaming numpy to np makes it easier to reference numpy functions and objects throughout your code.

import cv2
for i in glob.glob("/content/drive/MyDrive/Newbankdataset/*/*/*.png"):
  image=cv2.imread(i)
  cv2.imwrite(i.replace(".png",".jpg"),image)

Import statement: 
This line imports the cv2 module, which is the Python interface for OpenCV (Open Source Computer Vision Library).
OpenCV is a popular library used for image processing tasks such as reading, manipulating, and saving images, 
as well as performing computer vision operations.

for i in glob.glob("/content/drive/MyDrive/bankdataset/*/*/*.png"):

for loop: This line starts a for loop that iterates over a list of file paths generated by glob.glob("/content/drive/MyDrive/bankdataset/*/*/*.png").
glob.glob() is a function from the glob module (previously imported) that searches for all file paths matching a specified pattern. In this case, 
it searches for all .png files within the directory structure /content/drive/MyDrive/bankdataset/.

image = cv2.imread(i)

cv2.imread() function: This function from the cv2 module reads an image from the specified file path (i) and returns it as a NumPy array.
i is the current file path in the iteration of the for loop, representing a .png image file.

image: After executing cv2.imread(i), image now holds the contents of the image file i as a NumPy array.

cv2.imwrite(i.replace(".png", ".jpg"), image)--
  cv2.imwrite() function: This function from the cv2 module saves an image (image) to a specified file path (i.replace(".png", ".jpg")).
i.replace(".png", ".jpg"): This modifies the original file path i by replacing .png with .jpg.
This ensures that the image is saved with a .jpg extension instead of .png.
image: The NumPy array (image) representing the image data read earlier is written to the new file path.

this script converts all .png images found in the specified directory structure into .jpg images using OpenCV (cv2). 
It demonstrates how to read images, manipulate file paths, and save images using the cv2 module in Python.

import cv2
for i in glob.glob("/content/drive/MyDrive/bankdataset/*/*/*.jpg"):
  image=cv2.imread(i)
  cv2.imwrite(i.replace(".JPG",".jpg"),image)


this script converts all .jpg images found in the specified directory structure into .jpg images (with a lowercase extension) using OpenCV (cv2).
It demonstrates how to read images, manipulate file paths, 
and save images using the cv2 module in Python. The replacement of .JPG with .jpg ensures consistency in file naming and format.


print(len(glob.glob("/content/drive/MyDrive/Newbankdataset/*/*/*.jpg")))-

   the provided code snippet prints the number of .jpg files found in the specified directory structure (/content/drive/MyDrive/Newbankdataset/). 
It utilizes the glob.glob() function to search for files matching the pattern and the len() function to determine the count of matching files.
This can be useful for quickly verifying the number of files in a directory or preparing for batch operations on the files.


import glob
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

glob:
  The glob module is used for searching file pathnames matching a specified pattern.
    It provides a function called glob.glob() that returns a list of paths matching the specified pattern.
    Commonly used for file operations, particularly when you need to work with multiple files that match a certain criteria.
    Example usage: files = glob.glob("/path/to/directory/*.txt") - This would return a list of all .txt files in the specified directory.

numpy-
Import statement: This line imports the numpy library and assigns it the alias np. numpy is a powerful library for numerical computing in Python.
 It provides support for large, multi-dimensional arrays and 
matrices, along with a collection of mathematical functions to operate on these arrays efficiently.

Matplot: This line imports the pyplot module from matplotlib and assigns it the alias plt. 
matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB.
 It is used for creating static, animated, and interactive visualizations in Python.

Seaborn-This line imports the seaborn library and assigns it the alias sns. seaborn is a Python visualization library based on matplotlib. 
It provides a high-level interface for drawing attractive and informative statistical graphics.

glob()-
Used for file path pattern matching using the glob.glob() function.

numpy (np):
Used for numerical computing with support for large arrays and matrices, along with mathematical functions.

matplotlib.pyplot (plt):
Used for creating static visualizations like plots, histograms, scatterplots, etc.

seaborn (sns):
Used for statistical data visualization, providing a higher-level interface compared to matplotlib for creating more attractive
and informative plots.

!sudo apt install tesseract-ocr-sudo(Super user do)-
The command sudo apt install tesseract-ocr installs Tesseract, an open-source Optical Character Recognition (OCR) engine,
on your system using the Advanced Package Tool (APT). 
This allows your machine to convert images of text into machine-encoded text.


pip install pytesseract installs PyTesseract, a Python wrapper for the Tesseract OCR engine. This 
allows you to use Tesseract's OCR capabilities within Python scripts to extract text from images.

data=[]
for i in files:
  # print(i)
  extractedInformation = pytesseract.image_to_string(i)
  data.append([i,extractedInformation])

data=[]:
This initializes an empty list called data. It will be used to store the filenames and their corresponding extracted text.

for i in files::
This starts a for loop that iterates over each item in the files list. 
Here, files is expected to be a list containing the names or paths of image files.

extractedInformation = pytesseract.image_to_string(i):

This line uses PyTesseract to perform OCR on the image file specified by i.
The pytesseract.image_to_string(i) function reads the image file and extracts any text it finds, 
storing the result in the variable extractedInformation.

This appends a list containing the current filename i and the extracted text extractedInformation to the data list. 
Essentially, it adds a new entry to data for each file, where each entry is a list with two elements: the filename and the extracted text.


import pandas as pd:

This imports the pandas library and assigns it the alias pd. pandas is a powerful data manipulation and analysis library in Python.
data=pd.DataFrame(data):

This creates a DataFrame -
(a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure) from the data list.
 Here, data should already contain the list of lists (each inner
 list having the filename and extracted text), and it converts this list into a pandas DataFrame.

data=pd.read_csv("/content/data.csv"):

This reads a CSV file located at "/content/data.csv" into a pandas DataFrame and assigns it to the variable data. 
The CSV file is expected to be in a standard format, with rows representing data entries and columns representing data attributes.

dataclasses=[i.split("/")[-2] for i in files]
total_classes=np.array(dataclasses)
total_classes[0:5]

dataclasses = [i.split("/")[-2] for i in files]:

This is a list comprehension that creates a new list called dataclasses. It iterates over each item i in the files list.
i.split("/") splits the string i (which is expected to be a file path) into parts using the forward slash / as the delimiter.
\
[-2] accesses the second-to-last part of the split path. This is typically the directory name containing the file,
assuming the file path follows a standard structure (e.g., /path/to/classname/filename).
As a result, dataclasses will contain the directory names (or class names) for each file path in files.

total_classes = np.array(dataclasses):
This converts the dataclasses list into a NumPy array named total_classes.
 NumPy is a powerful library for numerical computing in Python, and
converting the list to an array can be useful for efficient numerical operations and manipulations.

total_classes[0:5]:
This retrieves the first five elements of the total_classes array. The slicing operation [0:5] 
selects elements from index 0 to index 4 (inclusive), providing a subset of the array.

data['label'] = total_classes:
This line adds a new column named label to the data DataFrame

data['label'].shape[0]:
This line retrieves the number of elements (or rows) in the label column of the data DataFrame.
shape[0] extracts the first element of the tuple, which gives the number of rows in the label column.

The .shape attribute of a DataFrame or Series returns a tuple. For a DataFrame, it's (number_of_rows, number_of_columns).
For a Series, it's (number_of_rows,).data['label'].shape[0] accesses the first element of the shape tuple, which represents
the number of rows in the label column.

!pip install wordcloud
wordcloud:
wordcloud is the name of the package you want to install. 
The wordcloud package allows you to create word cloud visualizations, which are graphical representations of word frequency.
The size of each word in the visualization is proportional to its frequency in the given text.

!pip install demoji
!pip install contractions
!pip install unidecode
!pip install num2words

demoji- is used to find and remove emojis from text.
contractions- is used to expand contractions in text (e.g., "can't" to "cannot").
unidecode- converts Unicode text to plain ASCII.
num2words- converts numbers to words in text.

data['text']=data['text'].astype(str)
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
wordcloud = WordCloud(background_color='white',  width=1600, height=800, random_state = 42).generate(' '.join(data['text']))
# plot the word cloud
plt.figure(figsize=(20,10), frameon=False)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

data['text'] = data['text'].astype(str):

This line converts the data in the data['text'] column to string type (str).
It ensures that all entries in the column are treated as strings, which is necessary for text processing operations like generating a word cloud.

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator:
This imports necessary components from the wordcloud module:
WordCloud: This class is used to generate word clouds.
STOPWORDS: A set of common words (like "the", "and", "is") that are typically filtered out before generating a word cloud.
ImageColorGenerator: This class allows the color of the word cloud to be based on an input image, although it's not used explicitly in your example.

wordcloud = WordCloud(background_color='white', width=1600, height=800, random_state=42).generate(' '.join(data['text'])):

This line creates a WordCloud object named wordcloud.
Parameters:
background_color='white': Sets the background color of the word cloud to white.
width=1600, height=800: Defines the dimensions (in pixels) of the word cloud image.
random_state=42: Sets a seed for random number generation to ensure reproducibility.
.generate(' '.join(data['text'])): This method generates the word cloud from the concatenated string of all text entries in the data['text'] column.

plt.figure(figsize=(20,10), frameon=False):
This line initializes a new figure for plotting with a specific size (figsize=(20,10)) and turns off the frame (frameon=False).

plt.imshow(wordcloud):
This displays the word cloud image created earlier (wordcloud) using matplotlib's imshow() function

plt.axis("off"):
This turns off the axes of the plot, hiding the x and y axis labels.

plt.tight_layout(pad=0):
This adjusts the spacing between subplots or figures automatically to fit within the figure area (pad=0 ensures minimal padding

plt.show():
This displays the plot. In this case, it shows the word cloud visualization generated based on the text data in data['text']



import pandas as pd
import numpy as np
import pandas as pd
import re
import nltk
import spacy
import string
import demoji
import contractions
import unidecode
from num2words import num2words
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup





import pandas as pd:
Imports the pandas library, used for data manipulation and analysis, and assigns it the alias pd for easier usage.

import numpy as np:
Imports the NumPy library, providing support for large, multi-dimensional arrays and matrices, and assigns it the alias np.

import re:
Imports the re module, providing support for working with regular expressions in Python, enabling text search and manipulation capabilities.

import nltk:
Imports the Natural Language Toolkit (NLTK), a platform for building Python programs to work with human language data, 
such as tokenization and stemming.

import spacy:
Imports the spaCy library, used for advanced natural language processing tasks, including tokenization, 
part-of-speech tagging, and named entity recognition.

import string:
Imports the string module, providing a collection of string constants and utilities, such as lists of ASCII letters, 
digits, and punctuation symbols.

import demoji:
Imports the demoji library, which allows detection and removal of emojis from text data.

import contractions:
Imports the contractions library, used to expand contractions (e.g., "can't" to "cannot") in English text.

import unidecode:
Imports the unidecode library, which transliterates Unicode text into ASCII characters, useful for text normalization.

from num2words import num2words:
Imports the num2words function from the num2words library, which converts numbers into words, helpful for textual representation of numeric data.

from nltk.corpus import stopwords:
Imports stopwords from the NLTK corpus, providing a list of common words (like "the", "and", "is") that are often filtered out in 
natural language processing tasks.

from nltk.stem.porter import PorterStemmer:
Imports the PorterStemmer class from the NLTK library, used for stemming words by removing affixes to produce the word stem.

Stemming:
In NLP, stemming is the process of reducing words to their root form. 
For example, stemming converts words like "running", "ran", and "runs" to their common root "run".

PorterStemmer:
The PorterStemmer algorithm, developed by Martin Porter, 
is a popular stemming algorithm in NLP.
It follows a set of rules to remove common prefixes and suffixes from words, aiming to produce the stem (root) of a word.


from nltk.stem.snowball import SnowballStemmer:
Imports the SnowballStemmer class from the NLTK library, which supports stemming in multiple languages.

from nltk.stem import WordNetLemmatizer:
Imports the WordNetLemmatizer class from the NLTK library, used for lemmatization, which reduces words to their base or root form.

In NLP tasks such as text mining, information retrieval, and sentiment analysis, lemmatization helps improve accuracy by reducing words
to their canonical forms.This facilitates better understanding of text data and enhances the performance of machine learning models.
In essence, the WordNetLemmatizer class in NLTK is a powerful tool for lemmatization,
 providing a more precise and linguistically accurate way to reduce words to their base forms compared to simple stemming techniques.

from bs4 import BeautifulSoup:
Imports the BeautifulSoup class from the bs4 (Beautiful Soup) library, 
used for parsing HTML and XML documents, extracting data from them, and helping clean web data.

Summary
These imports set up the Python environment with essential libraries and tools for 
data preprocessing, natural language processing (NLP), and text mining tasks, 
facilitating tasks such as text cleaning, tokenization, stemming, lemmatization, and more.

 
f.text = df.text.str.lower()
df.head(2) 
df.text = df.text.str.replace(r'https?://\S+|www.\S+', '', regex=True)
df.head() 
PUNCTUATIONS = string.punctuation
def remove_punctuation(text):
return text.translate(str.maketrans('', '', PUNCTUATIONS)) 
df.text = df["text"].apply(lambda text: remove_punctuation(text))


df.text = df.text.str.lower():
Converts all text in the text column of the DataFrame df to lowercase.
This ensures that all text is standardized to lowercase, which can help in text processing tasks like tokenization and comparison.

df.head(2):
Displays the first two rows of the DataFrame df after converting the text to lowercase.
This is a method in pandas used to inspect the DataFrame and verify the changes made to the text column

df.text = df.text.str.replace(r'https?://\S+|www\.\S+', '', regex=True):
Uses regular expression (regex) to replace URLs starting with http, https, or www with an empty string ('') in the text column of df.
This removes hyperlinks or URLs from the text data, which is common preprocessing step in text analysis tasks.

df.head():
Displays the first few rows of the DataFrame df after removing URLs.
This helps verify the effectiveness of the regex operation and inspect the updated text column.

PUNCTUATIONS = string.punctuation:
Defines a variable PUNCTUATIONS that holds all punctuation symbols.
This uses the string module's punctuation attribute, which contains
 all standard punctuation symbols (e.g., !"#$%&'()*+,-./:;<=>?@[\]^_{|}~`).

def remove_punctuation(text): 
    return text.translate(str.maketrans('', '', PUNCTUATIONS)):
Defines a function remove_punctuation that takes a text parameter and 
removes all punctuation from it using Python's str.translate() method.
This function uses str.maketrans() to create a translation table that maps each punctuation symbol to None, 
effectively removing them from the text.


df.text = df["text"].apply(lambda text: remove_punctuation(text)):
Applies the remove_punctuation function to each element in the text column of the DataFrame df.
This removes punctuation from all text entries in the text column, transforming the DataFrame in place.


These lines of code demonstrate common text preprocessing steps using pandas and 
Python string manipulation methods (str.lower(), str.replace(), and str.translate()). 
They prepare text data by converting it to lowercase, removing URLs, and stripping punctuation,
 making it cleaner and more suitable for further text analysis or machine learning tasks.


df.head()
function is used here to show the first few rows of the DataFrame df after these transformations


import nltk
nltk.download('stopwords')

The line nltk.download('stopwords') downloads the stopwords corpus from the NLTK (Natural Language Toolkit) library.

df.text = df.text.apply(lambda text: remove_stopwords(text))
df.head()
df.text = df.text.apply(lambda text: remove_stopwords(text)) applies a function (remove_stopwords)
 to remove common stopwords from each text entry in the text column of the DataFrame df

stemmer = PorterStemmer()
def stem_words(text):
return ' '.join([stemmer.stem(word) for word in text.split()])


stemmer = PorterStemmer():
Creates an instance of the PorterStemmer from NLTK, which is used for stemming words.

def stem_words(text)::
Defines a function named stem_words that takes a text parameter, which will be processed to stem each word

return ' '.join([stemmer.stem(word) for word in text.split()]):
Splits the input text into individual words, stems each word using the stemmer object (PorterStemmer), 
and then joins the stemmed words back into a single string separated by spaces.

df['text_stemmed'] = df.text.apply(lambda text: stem_words(text)):
Creates a new column text_stemmed in the DataFrame df.
Applies the stem_words function (which uses PorterStemmer from NLTK) to each entry in the text column.
The lambda function lambda text: stem_words(text) iterates through each text entry in the text column, 
stems each word, and returns the stemmed text.


df[['text', 'text_stemmed']].head(20):
Displays the first 20 rows of the DataFrame df, showing the original text column and the newly created text_stemmed column side by side.
This allows you to compare the original text with its stemmed version, showcasing the effect of stemming on the text data.

The SnowballStemmer.languages 
attribute provides a list of supported languages for stemming using the Snowball algorithm within the NLTK library.


lemmatizer = WordNetLemmatizer()
def text_lemmatize(text):
return ' '.join([lemmatizer.lemmatize(word) for word in text.split()]) 


lemmatizer = WordNetLemmatizer():
Creates an instance of the WordNetLemmatizer class from NLTK.
WordNetLemmatizer is used for lemmatization, which reduces 
words to their base or dictionary form (lemma), considering the context and meaning of the word

def text_lemmatize(text)::
Defines a function named text_lemmatize that takes a text parameter, which represents the text to be lemmatized.

return ' '.join([lemmatizer.lemmatize(word) for word in text.split()]):

Splits the input text into individual words using text.split().
Applies the lemmatizer.lemmatize() method to each word to find its lemma (base form).
Joins the lemmatized words back into a single string with spaces separating them using ' '.join().

The function text_lemmatize uses the WordNetLemmatizer from NLTK to process text by splitting it into words, 
lemmatizing each word to its base form, and then rejoining the lemmatized words into a coherent text string.
 This preprocessing step is crucial in NLP tasks to standardize words and 
improve the accuracy of text analysis and machine learning models by reducing the vocabulary size and handling 
different forms of words uniformly.


import nltk:

Imports the NLTK (Natural Language Toolkit) library, which is a popular Python library used for 
working with human language data, such as tokenization, stemming, lemmatization, parsing, and more.
nltk.download('wordnet'):

Downloads the WordNet corpus from NLTK.
WordNet is a lexical database for English that provides semantic relationships between words, 
including synonyms, hypernyms (more general terms), hyponyms (more specific terms), 
and meronyms (part-whole relationships).

This corpus is essential for tasks like lemmatization, where understanding word 
relationships helps determine the base or dictionary form of a word.
nltk.download('omw-1.4'):

Downloads the OMW (Open Multilingual WordNet) corpus version 1.4 from NLTK.
OMW extends the WordNet concept to multiple languages, providing similar semantic relationships
 for languages other than English.
It includes synsets (sets of synonyms) and other relationships in various languages, making it useful 
for multilingual NLP tasks and research.


t NLTK has access to essential lexical resources (WordNet and OMW) required for advanced text processing
 and analysis tasks, particularly for tasks involving semantics, multilingual support, and lemmatization.


!pip install catboost: Installs the CatBoost library, a gradient boosting algorithm-based
 machine learning library designed for handling categorical features efficiently.

!pip install river: 
!pip install river: This command installs the River library, which is specialized for online machine learning tasks. 
It's designed to handle continuous streams of data and supports incremental learning, 
where models can be updated continuously as new data arrives, making it efficient for real-time applications and large datasets.

from sklearn.feature_extraction.text import TfidfVectorizer

TF-IDF Vectorizer:
The TfidfVectorizer is a tool from Scikit-learn used for converting a collection of raw documents (text data) 
into a matrix of TF-IDF features.

TF-IDF (Term Frequency-Inverse Document Frequency):
TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection (corpus) of documents.
It balances the frequency of a word (term frequency, TF) within a document against how commonly
it appears across all documents (inverse document frequency, IDF).


x=df['text']
y=df['label']
tfidf=TfidfVectorizer()
tfidf.fit(x)
tfidf.transform(x) 


Certainly! Here's a concise explanation for each line:

x = df['text']: Assigns the 'text' column of the DataFrame df to the variable x, which holds the text data.

y = df['label']: Assigns the 'label' column of the DataFrame df to the variable y, which holds the corresponding labels or classes.

tfidf = TfidfVectorizer(): Initializes an instance of TfidfVectorizer from Scikit-learn,
 which will be used to convert text data into TF-IDF feature vectors.

tfidf.fit(x): Fits the TfidfVectorizer on the text data x, learning the vocabulary and 
IDF (Inverse Document Frequency) weights from the text corpus.

tfidf.transform(x): Transforms the text data x into a TF-IDF representation based on the 
fitted TfidfVectorizer, producing a sparse matrix where each row corresponds to a document and 
each column corresponds to a TF-IDF-weighted feature.

Summary:
These lines of code prepare text data (x) and its corresponding labels (y) for machine learning tasks:

x contains the raw text data.
y contains the corresponding labels or classes.
tfidf is a TfidfVectorizer object used to transform x into numerical TF-IDF features.
tfidf.fit(x) learns the vocabulary and IDF weights.
tfidf.transform(x) applies the learned vocabulary and IDF weights to convert x into 
TF-IDF feature vectors suitable for machine learning models.


x_vector = tfidf.transform(x) applies the fitted TfidfVectorizer (tfidf) to
 transform the raw text data x into a matrix of TF-IDF features.


from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
le.fit(y)
y=le.transform(y)

LabelEncoder converts categorical labels into numerical representations, 
making it suitable for supervised learning tasks where algorithms require numerical inputs.
fit(y) learns the mapping from categorical labels to numerical values.
transform(y) applies this mapping to convert categorical labels in y into numerical representations.
Together, these steps preprocess y for use in machine learning models that cannot directly handle categorical data.


from sklearn.model_selection import train_test_split
import tensorflow as tf
seed_value = 42
np.random.seed(seed_value)
random.seed(seed_value)
tf.random.set_seed(seed_value)
X_train, X_test, y_train, y_test = train_test_split(x_vector, y, test_size=0.33, random_state=seed_value)



from sklearn.model_selection import train_test_split:
Imports the train_test_split function from Scikit-learn, which is used to split arrays or matrices into random train and test subsets.

import tensorflow as tf:
Imports the TensorFlow library, a popular framework for building and training machine learning and deep learning models.

seed_value = 42:
Sets a seed value of 42 for reproducibility. This ensures that random operations produce the same results each time the code is run.

np.random.seed(seed_value):
Sets the seed for NumPy's random number generator to 42, ensuring that any random operations using NumPy will be reproducible.

random.seed(seed_value):
Sets the seed for Python's built-in random module to 42, ensuring reproducibility for any random operations using this module.

tf.random.set_seed(seed_value):
Sets the seed for TensorFlow's random number generator to 42, ensuring reproducibility for TensorFlow-related random operations.

X_train, X_test, y_train, y_test = train_test_split(x_vector, y, test_size=0.33, random_state=seed_value):
Splits the data into training and testing sets.

x_vector (the feature matrix) and y (the target labels) are divided into:

X_train: Training features (approximately 67% of the data).
X_test: Testing features (approximately 33% of the data).
y_train: Training labels.
y_test: Testing labels.
test_size=0.33 specifies that 33% of the data should be used for testing.
random_state=seed_value ensures the split is reproducible by using the same seed value.

Summary:
These lines of code set up the environment for reproducibility, split the data into training and testing sets,
and ensure that all random operations produce consistent results every time the code is executed.
This is essential for developing and evaluating machine learning models consistently.



X_train.shape = (148, 5938):

X_train is the training feature set.
It has 148 samples (documents).
Each sample is represented by 5938 features (words or terms in the TF-IDF vectorized space).
X_test.shape = (73, 5938):

X_test is the testing feature set.
It has 73 samples (documents).
Each sample is represented by 5938 features.
y_train.shape = (148,):

y_train is the training labels set.
It contains 148 labels, one for each training sample in X_train.
y_test.shape = (73,):

y_test is the testing labels set.
It contains 73 labels, one for each testing sample in X_test.


148 training samples: Each with 5938 features.
73 testing samples: Each with 5938 features.
148 training labels: Corresponding to the 148 training samples.
73 testing labels: Corresponding to the 73 testing samples.
This means the dataset has been split into training and testing subsets, 
with features (TF-IDF vectors) and corresponding labels for each sample.



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, precision_score, recall_score, f1_score
import lightgbm as lgb
import catboost as cbt
import xgboost as xgb
import time
from river import stream
from statistics import mode

Certainly! Here is a brief explanation of each line of code:
import pandas as pd:
Imports the pandas library, which is used for data manipulation and analysis, providing data structures like DataFrames.

import numpy as np:
Imports the NumPy library, which provides support for large, multi-dimensional arrays and matrices,
 along with a collection of mathematical functions to operate on these arrays.

import matplotlib.pyplot as plt:
Imports matplotlib.pyplot, a plotting library used for creating static, interactive, and animated visualizations in Python.

import seaborn as sns:
Imports the seaborn library, which is built on top of matplotlib and provides a high-level interface 
for drawing attractive and informative statistical graphics.

from sklearn.model_selection import train_test_split:
Imports the train_test_split function from Scikit-learn, which is used to split arrays or matrices into random train and 
test subsets for model evaluation.

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score:
Imports various evaluation metrics from Scikit-learn to assess the performance of classification models:

classification_report: Generates a report showing the main classification metrics.
confusion_matrix: Computes the confusion matrix to evaluate the accuracy of a classification.
accuracy_score: Calculates the accuracy of the model.
precision_score: Computes the precision of the model.
recall_score: Computes the recall of the model.
f1_score: Computes the F1 score, which is the harmonic mean of precision and recall.

import lightgbm as lgb:
Imports the LightGBM library, a gradient boosting framework that uses tree-based learning algorithms, optimized for speed and efficiency.

import catboost as cbt:
Imports the CatBoost library, a gradient boosting framework that handles categorical features automatically and efficiently.

import xgboost as xgb:
Imports the XGBoost library, an optimized gradient boosting library designed to be highly efficient, flexible, and portable.

import time:
Imports the time module, which provides various time-related functions, including the ability to measure execution time.

from river import stream:
Imports the stream module from the River library, which is designed for online machine learning, handling data
streams and incremental learning tasks.

from statistics import mode:
Imports the mode function from the statistics module, which returns the most common data point in a list or dataset.

Summary:
This block of code imports various libraries and functions required for data manipulation (pandas, NumPy), 
visualization (matplotlib, seaborn), data splitting and evaluation (Scikit-learn), machine learning model 
implementation (LightGBM, CatBoost, XGBoost), timing operations (time), handling data streams (River), and 
statistical calculations (statistics). These imports set up the environment for a comprehensive machine learning workflow.



%%time
# Train the Random Forest Model algorithm
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
print(classification_report(y_test,y_pred))
print("Accuracy of Random Forest: "+ str(accuracy_score(y_test, y_pred)))
print("Precision of Random Forest: "+ str(precision_score(y_test, y_pred, average='weighted')))
print("Recall of Random Forest: "+ str(recall_score(y_test, y_pred, average='weighted')))
print("Average F1 of Random Forest: "+ str(f1_score(y_test, y_pred, average='weighted')))
print("F1 of Random Forest for each type of attack: "+ str(f1_score(y_test, y_pred, average=None)))
rf_f1=f1_score(y_test, y_pred, average=None)

# Plot the confusion matrix
cm=confusion_matrix(y_test,y_pred)
f,ax=plt.subplots(figsize=(5,5))
sns.heatmap(cm,annot=True,linewidth=0.5,linecolor="red",fmt=".0f",ax=ax)
plt.xlabel("y_pred")
plt.ylabel("y_true")
plt.show()



%%time
This is a Jupyter notebook magic command that measures the wall time needed to execute the cell. 
It shows how long the code inside the cell takes to run.

from sklearn.ensemble import RandomForestClassifier
Imports the RandomForestClassifier class from Scikit-learn, which is an ensemble learning method for classification 
that operates by constructing multiple decision trees.

rf = RandomForestClassifier()
Creates an instance of the RandomForestClassifier. This object (rf) will be used to train the random forest model.
python
Copy code
rf.fit(X_train, y_train)
Fits the random forest model using the training data (X_train and y_train). This step 
involves training the model to learn from the training data.

y_pred = rf.predict(X_test)
Uses the trained random forest model to make predictions on the test data (X_test). The predicted labels are stored in y_pred.

print(classification_report(y_test,y_pred))
Prints a detailed classification report that includes precision, recall, f1-score, and support for each class.

print("Accuracy of Random Forest: " + str(accuracy_score(y_test, y_pred)))
Calculates and prints the accuracy of the random forest model by comparing the true labels (y_test) with the predicted labels (y_pred).

print("Precision of Random Forest: " + str(precision_score(y_test, y_pred, average='weighted')))
Calculates and prints the precision of the random forest model. The average='weighted' parameter means that the precision
 score is calculated for each label, and their average is weighted by the number of true instances for each label.

print("Recall of Random Forest: " + str(recall_score(y_test, y_pred, average='weighted')))
Calculates and prints the recall of the random forest model using the weighted average.

print("Average F1 of Random Forest: " + str(f1_score(y_test, y_pred, average='weighted')))
Calculates and prints the average F1-score of the random forest model using the weighted average.

print("F1 of Random Forest for each type of attack: " + str(f1_score(y_test, y_pred, average=None)))
Calculates and prints the F1-score of the random forest model for each class individually.

rf_f1 = f1_score(y_test, y_pred, average=None)
Stores the F1-scores for each class individually in the variable rf_f1.

cm = confusion_matrix(y_test, y_pred)
Computes the confusion matrix, which shows the counts of true positive, true negative, false positive, and false negative predictions for each class.

f, ax = plt.subplots(figsize=(5, 5))
Creates a new figure and axis with a size of 5x5 inches for plotting.

sns.heatmap(cm, annot=True, linewidth=0.5, linecolor="red", fmt=".0f", ax=ax)
Uses Seaborn to create a heatmap of the confusion matrix (cm). The heatmap has annotations showing the counts, a line width of 0.5, red lines between cells, and integer formatting for the annotations. The plot is drawn on the axis ax.

plt.xlabel("y_pred")
Sets the label for the x-axis of the plot to "y_pred".

plt.ylabel("y_true")
Sets the label for the y-axis of the plot to "y_true".

plt.show()
Displays the plot.




#importing confusion matrix
from sklearn.metrics import confusion_matrix
confusion = confusion_matrix(y_test, y_pred)
print('Confusion Matrix\n')
print(confusion)

#importing accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y_test, y_pred)))

print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2','class 3','class 4']))


from sklearn.metrics import confusion_matrix
Imports the confusion_matrix function from Scikit-learn, which is used to compute the confusion matrix to evaluate the accuracy of a classification.

confusion = confusion_matrix(y_test, y_pred)
Computes the confusion matrix by comparing the true labels (y_test) with the predicted labels (y_pred) and stores it in the variable confusion.

print('Confusion Matrix\n')
print(confusion)
Prints the text "Confusion Matrix" followed by the confusion matrix itself.

#importing accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
Imports the accuracy, precision, recall, and F1 score functions from Scikit-learn for evaluating classification models.

print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y_test, y_pred)))
Calculates and prints the accuracy of the model with two decimal points of precision.

print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))
Calculates and prints the micro-averaged precision of the model, which aggregates the contributions of all classes to compute the average metric.

print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))
Calculates and prints the micro-averaged recall of the model.

print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='micro')))
Calculates and prints the micro-averaged F1 score of the model.

print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))
Calculates and prints the macro-averaged precision of the model, which computes the metric independently for each class and then 
takes the average (treats all classes equally).

print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))
Calculates and prints the macro-averaged recall of the model.

print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='macro')))
Calculates and prints the macro-averaged F1 score of the model.

print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))
Calculates and prints the weighted precision of the model, which averages the metrics for each class, weighted by the 
number of true instances for each class.

print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))
Calculates and prints the weighted recall of the model.

print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))
Calculates and prints the weighted F1 score of the model.

from sklearn.metrics import classification_report
Imports the classification_report function from Scikit-learn, which provides a detailed report showing the main classification 
metrics for each class.

print('\nClassification Report\n')

print(classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4']))

Prints the text "Classification Report" followed by the detailed classification report, which includes precision, recall, 
F1 score, and support for each specified class. The target_names parameter is used to label the classes in the
report as 'Class 1', 'Class 2', 'Class 3', and 'Class 4'.


from sklearn.ensemble import VotingClassifier
clf1=xgb.XGBClassifier()
clf2=  cbt.CatBoostClassifier()
clf3=  RandomForestClassifier()

#### Ensemble
eclf1 = VotingClassifier(estimators=[('cat', clf1), ('sv', clf2),('GB',clf3)],voting='soft')
eclf1 = eclf1.fit(X_train, y_train)


from sklearn.ensemble import VotingClassifier
Imports the VotingClassifier class from Scikit-learn, which is used to combine several different machine learning classifiers into an ensemble model that makes predictions based on a majority vote or the average predicted probabilities.

clf1 = xgb.XGBClassifier()
Creates an instance of the XGBClassifier from the XGBoost library, which is a powerful gradient boosting model.

clf2 = cbt.CatBoostClassifier()
Creates an instance of the CatBoostClassifier from the CatBoost library, which is another gradient boosting model that handles categorical features well.

clf3 = RandomForestClassifier()
Creates an instance of the RandomForestClassifier from Scikit-learn, which is an ensemble method that uses multiple decision trees to make predictions.

#### Ensemble
eclf1 = VotingClassifier(estimators=[('cat', clf1), ('sv', clf2), ('GB', clf3)], voting='soft')
Creates an instance of the VotingClassifier called eclf1, which combines the three classifiers (clf1, clf2, and clf3).

The estimators parameter specifies the list of classifiers to include in the ensemble, each labeled with a unique identifier ('cat', 'sv', 'GB').
The voting='soft' parameter means that the ensemble will make predictions based on the average of predicted probabilities from each classifier (soft voting).

eclf1 = eclf1.fit(X_train, y_train)
Fits the VotingClassifier ensemble model on the training data (X_train and y_train). This step involves training each of the individual classifiers within the ensemble on the training data.



































